{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "steinmetz_analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mmcinnestaylor/NMA-CN-2022/blob/main/project/steinmetz_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LfDG_mHHVKsv"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "PCcEl-7AVp58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_PATH = '/content/drive/MyDrive/steinmetz' # Path to dataset base directory\n",
        "\n",
        "WINDOW_SIZE = 10 #will be used for binning spikes"
      ],
      "metadata": {
        "id": "oZYfViQqF_0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Helper Functions\n",
        "#@markdown - get_spike_trains(clustered_spikes, start_time=0, bin_size=10, window_size=100, format='binary')\n",
        "def get_spike_trains(clustered_spikes, start_time=0, bin_size=10, window_size=100, format='binary'):\n",
        "  \"\"\"\n",
        "  Spike train generator function.\n",
        "  Builds trains over a given time window using discrete time bins. \n",
        "\n",
        "  Args:\n",
        "    clustered_spikes : a dictionary of clusters\n",
        "    start_time(ms)   : the starting time point of the window within a session\n",
        "    bin_size(ms)     : the size of the discrete time step within the window\n",
        "    window_size(ms)  : the width of the time window\n",
        "    window_size(ms)  : binary or counts\n",
        "\n",
        "  Returns:\n",
        "    spike_trains     : the spikes trains of the neurons in the window\n",
        "  \"\"\"\n",
        "  \n",
        "  num_neurons = len(clustered_spikes.keys())\n",
        "  num_bins = int(window_size/bin_size)\n",
        "\n",
        "  if format == 'binary' or format == 'counts':\n",
        "    # Initialize spike train matrix [num_neurons x num_bins]\n",
        "    spike_trains = np.zeros((num_neurons,num_bins), dtype=int)\n",
        "\n",
        "    # Iterate over all neurons in a recording session\n",
        "    for i,cluster_id in enumerate(sorted(clustered_spikes.keys())):\n",
        "      # Iterate over time bins in the window\n",
        "      for j in range(num_bins):\n",
        "        # Define bin start and end times\n",
        "        bin_start = start_time + (j * bin_size)\n",
        "        bin_end = start_time + ((j+1) * bin_size)\n",
        "        \n",
        "        # Convert spike times of a given neuron to millisecond scale\n",
        "        neuron_spikes = clustered_spike_times[cluster_id] * 1000 \n",
        "        \n",
        "        # Check if a spike occured in time bin [start, end)\n",
        "        spikes = np.logical_and(neuron_spikes>=bin_start, neuron_spikes<bin_end)\n",
        "        if format == 'binary':\n",
        "          if True in spikes:\n",
        "            spike_trains[i][j] = 1\n",
        "        else:\n",
        "          spike_trains[i][j] = np.count_nonzero(spikes==True) \n",
        "  else: # to be implemented\n",
        "    pass\n",
        "    \n",
        "  return spike_trains"
      ],
      "metadata": {
        "id": "kVRDJAIvx64c",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Load Data\n",
        "#@markdown - Uses the path defined in BASE_PATH\n",
        "#@markdown - Assumes each recording session resides in a separate subdirectory formatted as mouseID_year-month-date\n",
        "#@markdown - **'mouse_id'**: (str) Mouse name per session directory format\n",
        "#@markdown - **'session_date'**: (date) Date of session per session directory format\n",
        "#@markdown - **'clustered_spikes'**: A dictionary of clusters(neurons) recorded during the session. Clusters with annotation values < 1 are not included.\n",
        "#@markdown  - keys(int): The cluster's integer ID as specified in the datafiles.\n",
        "#@markdown  - values(np.array): A 1-D array of size *nSpikes* where each entry corresponds to a time point in seconds during the recording session in which the cluster(neuron) produced a spike.\n",
        "#@markdown - **'clusters_peak_channels'**: (np.array) A 1-D  array of size *nClusters* where each entry corresponds to the channel number of the location of the peak of the cluster's waveform. The number maps to the brain region of the channel, using th Allen CCF.\n",
        "\n",
        "all_session_data = dict()\n",
        "\n",
        "# Order sessions by surname + date\n",
        "for i,session in tqdm(enumerate(sorted(os.listdir(BASE_PATH)))):\n",
        "  session_path = os.path.join(BASE_PATH, session)\n",
        "\n",
        "  print(f\"Loading session: {session} \")\n",
        "\n",
        "  # Load spike and cluster data\n",
        "  raw_spikes = np.load(session_path+'/spikes.times.npy')\n",
        "  raw_clusters = np.load(session_path+'/spikes.clusters.npy')\n",
        "  cluster_annotations = np.load(session_path+'/clusters._phy_annotation.npy'), #cluster quality\n",
        "  num_clusters = raw_clusters.max() \n",
        "\n",
        "  # Initialize 2-D list\n",
        "  sorted_spike_times = [[] for i in range(num_clusters+1)]\n",
        "\n",
        "  # Group spike times by their predicted cluster number \n",
        "  for j in range(len(raw_spikes)):\n",
        "    sorted_spike_times[raw_clusters[j][0]].append(raw_spikes[j][0])\n",
        "  \n",
        "  # Dict to store valid clusters (annotation quality > 1)\n",
        "  filtered_clusters = dict()\n",
        "\n",
        "  # Convert clustered spikes to numpy arrays for efficiency\n",
        "  for j,cluster in enumerate(sorted_spike_times):\n",
        "    # Only store valid clusters\n",
        "    if cluster_annotations[0][j][0] > 1:\n",
        "      filtered_clusters[j] = np.array(sorted_spike_times[j])\n",
        "\n",
        "  # Load session data into dictionary\n",
        "  all_session_data[i] = {\n",
        "    # Session information\n",
        "    'mouse_id': session.split('_')[0],\n",
        "    'session_date': datetime.strptime(session.split('_')[1], '%Y-%m-%d').date(),\n",
        "\n",
        "    # Neural Data \n",
        "    'clustered_spikes': filtered_clusters, # Dict: key=cluster_ID, val=spike_times(seconds)\n",
        "\n",
        "    # Cluster Data\n",
        "    'clusters_peak_channels': np.load(session_path+'/clusters.peakChannel.npy'),\n",
        "\n",
        "    # Channels Data\n",
        "    #'channels_site': np.load(session_path+'/channels.site.npy')\n",
        "  }"
      ],
      "metadata": {
        "id": "6PwfjOuKDpzp",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title File writer for Kobayashi method\n",
        "\n",
        "#num_clusters = len(sorted_spike_times.keys())\n",
        "\n",
        "with open(\"/content/drive/MyDrive/steinmetz/Cori_2016-12-14/kobayashi_datafile_5n.txt\", \"w\") as f:\n",
        "  for i,cluster in enumerate(sorted_spike_times.keys()):\n",
        "    written = False\n",
        "    \n",
        "    # check if cluster is valid \n",
        "    if spike_clusters_annotations[cluster][0] < 2:\n",
        "      continue\n",
        "\n",
        "    for time in sorted_spike_times[cluster]:\n",
        "      # limit spikes to first 5 minutes\n",
        "      if time <= 10:\n",
        "        written = True\n",
        "        f.write(str(time)+'\\n')\n",
        "\n",
        "    #if i != num_clusters:\n",
        "    # append only if a spike time has been written for a given neuron\n",
        "    if written:    \n",
        "      f.write(';\\n')\n",
        "  \n",
        "  f.close()"
      ],
      "metadata": {
        "id": "DaTZwFISn6nA",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(all_session_data[0]['clustered_spikes'][0][:50])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acyeOZQ9f_nh",
        "outputId": "f8627007-4c02-4032-c72f-ca5b759a84b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  0.8149      14.82246667  24.9646      25.1436      38.8709\n",
            "  50.8208      54.80686667  59.51183333  80.47036667 127.09636667\n",
            " 166.78713333 175.2476     177.2883     178.31753333 231.78033333\n",
            " 240.66676667 271.28896667 305.5415     305.64986667 311.1602\n",
            " 312.75296667 318.46916667 322.8011     324.85113333 356.86103333\n",
            " 356.9972     377.01963333 388.1814     390.37333333 434.21206667\n",
            " 442.29883333 473.92586667 484.7502     503.32483333 503.3358\n",
            " 503.36313333 503.4157     530.48366667 574.1442     584.98683333\n",
            " 586.90366667 593.3421     614.83113333 654.2239     654.53083333\n",
            " 654.8104     719.13886667 739.67513333 744.6198     745.3653    ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = get_spike_trains(all_session_data[0]['clustered_spikes'], start_time=0, window_size=1000, bin_size=100, format='binary')"
      ],
      "metadata": {
        "id": "xdetzUPbBgh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train[0]"
      ],
      "metadata": {
        "id": "63kUhDn4BjDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = get_spike_trains(all_session_data[0]['clustered_spikes'], start_time=24000, window_size=4000, bin_size=2000, format='counts')"
      ],
      "metadata": {
        "id": "AhTJZ0CZcbYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84XrXfuKczh9",
        "outputId": "bb1e683a-d943-4cb8-b247-2496acd2d2ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    }
  ]
}